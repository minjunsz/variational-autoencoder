{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, ModelSummary, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from data_modules import MNIST\n",
    "from models.naive_vae import NaiveVAE\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate pl.Datamodule & pl.LightningModule\n",
    "\n",
    "Data dir should be consistent in order not to re-download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = MNIST.MNISTDataModule(\"./downloads\")\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "vae = NaiveVAE(in_channels=1, hidden_dim=[32,64,128,256], latent_dim=128, lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fast_dev_run\n",
    "\n",
    "Thanks to pytorch-lightning's fast_dev_run mode, the model would be trained based on a single batch.  \n",
    "ModelSummary callback prints the dimension of the intermediate results, which is estimated according to the example input.\n",
    "The example input, `example_input_array`, is define in the model's `__init__` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug train\n",
    "trainer = pl.Trainer(\n",
    "    fast_dev_run=True,\n",
    "    callbacks=[ModelSummary(max_depth=1)],\n",
    ")\n",
    "trainer.fit(model=vae, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit on small batch\n",
    "\n",
    "In order to check whether the model is expressive enough, it'd be better to train on small batches. The model should overfit on them quickly.  \n",
    "Learning rate can be adjusted, by tracking gradients while overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    default_root_dir=\"checkpoints/naive_vae\",\n",
    "    overfit_batches=10,\n",
    "    track_grad_norm=2,\n",
    "    max_epochs=500,\n",
    ")\n",
    "\n",
    "trainer.fit(model=vae, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(name='Adam-128-0.001',project='NaiveVAE', log_model='all')\n",
    "wandb_logger.watch(vae, log='all')\n",
    "\n",
    "val_checkpoint = ModelCheckpoint(monitor=\"val_total_loss\", mode=\"min\", save_top_k=2, filename=\"{epoch}-{step}-{val_total_loss:.3f}\")\n",
    "latest_checkpoint = ModelCheckpoint(monitor=\"step\", mode=\"max\", every_n_epochs=1, save_top_k=3, filename=\"latest-{epoch}-{step}\")\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=\"checkpoints/naive_vae\",\n",
    "    max_epochs=20,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[val_checkpoint, latest_checkpoint, lr_monitor],\n",
    ")\n",
    "trainer.fit(model=vae,datamodule=dm)\n",
    "wandb_logger.finalize('success')\n",
    "wandb.finish(exit_code=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
